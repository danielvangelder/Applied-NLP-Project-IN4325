{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "architectural-gathering",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "veterinary-finland",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/danielvangelder/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "from tqdm.notebook import tqdm\n",
    "# stopwords\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rapid-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_BODIES_PATH = \"data/fnc-1/competition_test_bodies.csv\"\n",
    "TEST_STANCES_PATH = \"data/fnc-1/competition_test_stances.csv\"\n",
    "TRAIN_BODIES_PATH = \"data/fnc-1/train_bodies.csv\"\n",
    "TRAIN_STANCES_PATH = \"data/fnc-1/train_stances.csv\"\n",
    "ALBERT_PREDICTIONS = \"data/fnc-1/golden_labels_2.csv\"\n",
    "BASELINE_PREDICTIONS = \"data/fnc-1/baseline_output.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "steady-twelve",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_merged_df(bodies_path, stances_path):\n",
    "    bodies = pd.read_csv(bodies_path, names=['Body ID', 'articleBody'], header=0)\n",
    "    stances = pd.read_csv(stances_path, names=['Headline', 'Body ID', 'Stance'], header=0)\n",
    "    df = pd.merge(bodies, stances, on='Body ID')\n",
    "    return df\n",
    "\n",
    "\n",
    "albert = pd.read_csv(ALBERT_PREDICTIONS, names=['Headline', 'Body ID', 'Stance'], header=0)\n",
    "baseline = pd.read_csv(BASELINE_PREDICTIONS, names=['Headline', 'Body ID', 'Stance'], header=0)\n",
    "baseline.columns = ['Headline', 'Body ID', 'Stance_baseline']\n",
    "test_res = create_merged_df(TEST_BODIES_PATH, TEST_STANCES_PATH)\n",
    "# test_res['albert'] = pd.malbert[['Headline', 'Stance']]\n",
    "test_res = pd.merge(test_res,albert, on=['Headline', 'Body ID'], suffixes=['_true', '_albert'])\n",
    "test_res = pd.merge(test_res,baseline, on=['Headline', 'Body ID'])\n",
    "train = create_merged_df(TRAIN_BODIES_PATH, TRAIN_STANCES_PATH)\n",
    "# display(test_res)\n",
    "# display(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "academic-brand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>correct_albert</th>\n",
       "      <th>correct_base</th>\n",
       "      <th>missed_albert</th>\n",
       "      <th>missed_base</th>\n",
       "      <th>total</th>\n",
       "      <th>prop_albert</th>\n",
       "      <th>prop_base</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Source: Joan Rivers' doc did biopsy, selfie</td>\n",
       "      <td>126</td>\n",
       "      <td>193</td>\n",
       "      <td>75</td>\n",
       "      <td>8</td>\n",
       "      <td>201</td>\n",
       "      <td>0.626866</td>\n",
       "      <td>0.960199</td>\n",
       "      <td>-67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Charles Manson’s fiancee allegedly wanted to m...</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Joan Rivers Personal Doctor Allegedly Took A S...</td>\n",
       "      <td>55</td>\n",
       "      <td>82</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>90</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Disturbed aunt cuts off nephew’s penis after h...</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>57</td>\n",
       "      <td>0.719298</td>\n",
       "      <td>0.719298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Joan Rivers’ Doctor Snapped Selfie During Thro...</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>Son's Asinine Questions Inspire Priceless Dad-...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>Here's What We Know About ISIS's Alleged Organ...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>9 Things You Need To Know About The Climate Ch...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>Heart specialist questions benefits of high-in...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>“I am lost for words,” 5-year-old boy billed f...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>893 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Headline  correct_albert  \\\n",
       "0          Source: Joan Rivers' doc did biopsy, selfie             126   \n",
       "1    Charles Manson’s fiancee allegedly wanted to m...              81   \n",
       "2    Joan Rivers Personal Doctor Allegedly Took A S...              55   \n",
       "3    Disturbed aunt cuts off nephew’s penis after h...              41   \n",
       "4    Joan Rivers’ Doctor Snapped Selfie During Thro...              36   \n",
       "..                                                 ...             ...   \n",
       "888  Son's Asinine Questions Inspire Priceless Dad-...               0   \n",
       "889  Here's What We Know About ISIS's Alleged Organ...               0   \n",
       "890  9 Things You Need To Know About The Climate Ch...               0   \n",
       "891  Heart specialist questions benefits of high-in...               0   \n",
       "892  “I am lost for words,” 5-year-old boy billed f...               0   \n",
       "\n",
       "     correct_base  missed_albert  missed_base  total  prop_albert  prop_base  \\\n",
       "0             193             75            8    201     0.626866   0.960199   \n",
       "1              81              0            0     81     1.000000   1.000000   \n",
       "2              82             35            8     90     0.611111   0.911111   \n",
       "3              41             16           16     57     0.719298   0.719298   \n",
       "4              36              0            0     36     1.000000   1.000000   \n",
       "..            ...            ...          ...    ...          ...        ...   \n",
       "888             1              1            0      1     0.000000   1.000000   \n",
       "889             6              7            1      7     0.000000   0.857143   \n",
       "890             2              5            3      5     0.000000   0.400000   \n",
       "891             0              4            4      4     0.000000   0.000000   \n",
       "892             0              5            5      5     0.000000   0.000000   \n",
       "\n",
       "     difference  \n",
       "0           -67  \n",
       "1             0  \n",
       "2           -27  \n",
       "3             0  \n",
       "4             0  \n",
       "..          ...  \n",
       "888          -1  \n",
       "889          -6  \n",
       "890          -2  \n",
       "891           0  \n",
       "892           0  \n",
       "\n",
       "[893 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_res_rel = test_res.loc[test_res['Stance_true'] != 'unrelated']\n",
    "correct = test_res_rel.copy()\n",
    "correct['correct_albert'] = test_res_rel['Stance_true'] == test_res_rel['Stance_albert']\n",
    "correct['correct_base'] = test_res_rel['Stance_true'] == test_res_rel['Stance_baseline']\n",
    "correct = correct[['articleBody', 'Headline', 'Stance_true', 'correct_albert', 'correct_base']]\n",
    "# display(correct)\n",
    "correct_count = correct[['Headline', 'correct_albert', 'correct_base']].groupby(['Headline']).sum().sort_values('correct_albert', ascending=False)\n",
    "correct_count.reset_index(level=0, inplace=True)\n",
    "# display(correct_count)\n",
    "pair_count = {}\n",
    "rel_headlines = set(test_res_rel['Headline'].values)\n",
    "# print(rel_headlines)\n",
    "for head in rel_headlines:\n",
    "    pair_count[head] = test_res_rel.loc[test_res_rel['Headline'] == head].shape[0]\n",
    "    \n",
    "grouped_res = correct_count.copy()\n",
    "missed_count_albert = []\n",
    "missed_count_base = []\n",
    "total = []\n",
    "\n",
    "for headline in grouped_res['Headline'].values:\n",
    "    total.append(pair_count[headline])\n",
    "    missed_count_albert.append(pair_count[headline] - grouped_res.loc[grouped_res['Headline'] == headline]['correct_albert'].values[0])\n",
    "    missed_count_base.append(pair_count[headline] - grouped_res.loc[grouped_res['Headline'] == headline]['correct_base'].values[0])\n",
    "     \n",
    "grouped_res['missed_albert'] = missed_count_albert\n",
    "grouped_res['missed_base'] = missed_count_base\n",
    "grouped_res['total'] = total\n",
    "\n",
    "grouped_res['prop_albert'] = grouped_res['correct_albert'] / grouped_res['total']\n",
    "grouped_res['prop_base'] = grouped_res['correct_base'] / grouped_res['total']\n",
    "\n",
    "grouped_res['difference'] = grouped_res['correct_albert'] - grouped_res['correct_base']\n",
    "display(grouped_res)\n",
    "# display(grouped_res.loc[grouped_res['total'] > 20].sort_values('prop_albert', ascending=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sporting-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_success = grouped_res.loc[grouped_res['prop_albert'] > 0.99]['Headline'].values\n",
    "headlines_failed = grouped_res.loc[grouped_res['prop_albert'] < 0.01]['Headline'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-defendant",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "excellent-birthday",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/danielvangelder/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "instrumental-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = {}\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            token_dict[result[-1]] = token\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "verbal-ferry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['Charles', 'Manson’s', 'fiancee', 'allegedly', 'wanted', 'to', 'marry', 'him', 'for', 'his', 'corpse']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['charl', 'manson', 'fiance', 'alleg', 'want', 'marri', 'corps']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = headlines_success[0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "sound-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_success_proc = list(map(preprocess, headlines_success))\n",
    "headlines_failed_proc = list(map(preprocess, headlines_failed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "informed-faith",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Journalist Steven Sotloff reportedly executed by ISIS ['journalist', 'steven', 'sotloff', 'report', 'execut', 'isi']\n",
      "‘You Were My Guy’: Trump Tells Macron He Supported Him During French Election ['trump', 'tell', 'macron', 'support', 'french', 'elect']\n"
     ]
    }
   ],
   "source": [
    "print(headlines_success[5],headlines_success_proc[5])\n",
    "print(headlines_failed[5],headlines_failed_proc[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-sunday",
   "metadata": {},
   "source": [
    "A very crude tokenizer indeed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "strategic-exhibit",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_success = gensim.corpora.Dictionary(headlines_success_proc)\n",
    "dictionary_success.filter_extremes(no_below=3, no_above=0.5, keep_n=100000)\n",
    "dictionary_failed = gensim.corpora.Dictionary(headlines_failed_proc)\n",
    "dictionary_failed.filter_extremes(no_below=3, no_above=0.5, keep_n=100000)\n",
    "bow_corpus_success = [dictionary_success.doc2bow(doc) for doc in headlines_success_proc]\n",
    "bow_corpus_failed = [dictionary_failed.doc2bow(doc) for doc in headlines_failed_proc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fantastic-closer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 17 (\"kill\") appears 1 time.\n",
      "Word 20 (\"isi\") appears 1 time.\n",
      "Word 54 (\"baghdadi\") appears 1 time.\n",
      "Word 55 (\"bakr\") appears 1 time.\n",
      "Word 57 (\"leader\") appears 1 time.\n",
      "Word 59 (\"airstrik\") appears 1 time.\n",
      "Word 61 (\"wound\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "t = 99\n",
    "for i in range(len(bow_corpus_success[t])):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_corpus_success[t][i][0], dictionary_success[bow_corpus_success[t][i][0]], \n",
    "bow_corpus_success[t][i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "announced-cleaning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic modelling success...\n",
      "Topic modelling failed...\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "print(\"Topic modelling success...\")\n",
    "lda_model_sucess = gensim.models.LdaMulticore(bow_corpus_success, num_topics=10, id2word=dictionary_success, passes=2, workers=4)\n",
    "print(\"Topic modelling failed...\")\n",
    "lda_model_failed = gensim.models.LdaMulticore(bow_corpus_failed, num_topics=10, id2word=dictionary_failed, passes=2, workers=4)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "vertical-bottom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['note', 'suicide', 'transgender', 'woman', 'teen', 'blaming', 'parents', 'christian', 'madonna', 'voters']\n",
      "['saudi', 'penis', 'acid', 'woman', 'arabia', 'airline', 'boyfriend', 'girl', 'pours', 'tape']\n",
      "['apple', 'watch', 'iphone', 'inch', 'launch', 'rumors', 'health', 'features', 'march', 'christmas']\n",
      "['afghan', 'missing', 'reports', 'soldiers', 'border', 'canadian', 'bakr', 'baghdadi', 'killed', 'apple']\n",
      "['jihadi', 'isis', 'airstrikes', 'john', 'reports', 'injured', 'nevada', 'tesla', 'year', 'letter']\n",
      "['reports', 'apple', 'blackberry', 'lenovo', 'rivers', 'joan', 'selfie', 'doctor', 'watch', 'corpse']\n",
      "['reports', 'split', 'companies', 'attacked', 'wounded', 'selfie', 'doctor', 'rivers', 'joan', 'packard']\n",
      "['reports', 'isis', 'million', 'sell', 'sotloff', 'beheads', 'apple', 'steven', 'foley', 'james']\n",
      "['claims', 'apple', 'fighters', 'watch', 'display', 'screens', 'supplying', 'samsung', 'photos', 'rumors']\n",
      "['reports', 'apple', 'plan', 'music', 'expansion', 'beats', 'vegas', 'isis', 'social', 'path']\n"
     ]
    }
   ],
   "source": [
    "# for idx, topic in lda_model_sucess.print_topics(-1):\n",
    "#     print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "for i in range(10):\n",
    "    print([token_dict[p[0]] for p in lda_model_sucess.show_topic(i, topn=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "current-oxford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lost', 'year', 'billed', 'missing', 'birthday', 'friend', 'party', 'ferguson', 'shot', 'woman']\n",
      "['climate', 'change', 'hackers', 'sony', 'reports', 'hoax', 'stolen', 'beheads', 'isis', 'claims']\n",
      "['threatened', 'kushner', 'isis', 'families', 'syria', 'channel', 'denies', 'wanted', 'state', 'wedding']\n",
      "['airstrikes', 'president', 'intensity', 'killed', 'reports', 'jihadi', 'argentina', 'werewolf', 'isis', 'russian']\n",
      "['penis', 'woman', 'reports', 'isis', 'claims', 'getting', 'stolen', 'boob', 'iraqi', 'year']\n",
      "['women', 'plan', 'apple', 'separate', 'birthday', 'billed', 'party', 'missing', 'friend', 'photos']\n",
      "['woman', 'breast', 'surgery', 'state', 'watch', 'apple', 'claims', 'weapons', 'told', 'denies']\n",
      "['isis', 'organ', 'harvesting', 'wedding', 'operation', 'boob', 'finance', 'iraqi', 'fake', 'accuses']\n",
      "['stop', 'saturated', 'heart', 'wikileaks', 'rich', 'seth', 'streets', 'weapons', 'claws', 'plan']\n",
      "['killed', 'reports', 'revealed', 'woman', 'fake', 'breast', 'buried', 'grave', 'zombie', 'claws']\n"
     ]
    }
   ],
   "source": [
    "# for idx, topic in lda_model_failed.print_topics(-1):\n",
    "#     print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "for i in range(10):\n",
    "    print([token_dict[p[0]] for p in lda_model_failed.show_topic(i, topn=10)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
