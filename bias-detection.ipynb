{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "another-system",
   "metadata": {},
   "source": [
    "# Data Loading/Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "# stopwords\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-conversation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-character",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_BODIES_PATH = \"data/fnc-1/competition_test_bodies.csv\"\n",
    "TEST_STANCES_PATH = \"data/fnc-1/competition_test_stances.csv\"\n",
    "TRAIN_BODIES_PATH = \"data/fnc-1/train_bodies.csv\"\n",
    "TRAIN_STANCES_PATH = \"data/fnc-1/train_stances.csv\"\n",
    "ALBERT_PREDICTIONS = \"data/fnc-1/golden_labels_2.csv\"\n",
    "BASELINE_PREDICTIONS = \"data/fnc-1/baseline_output.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_merged_df(bodies_path, stances_path):\n",
    "    bodies = pd.read_csv(bodies_path, names=['Body ID', 'articleBody'], header=0)\n",
    "    stances = pd.read_csv(stances_path, names=['Headline', 'Body ID', 'Stance'], header=0)\n",
    "    df = pd.merge(bodies, stances, on='Body ID')\n",
    "    return df\n",
    "\n",
    "\n",
    "albert = pd.read_csv(ALBERT_PREDICTIONS, names=['Headline', 'Body ID', 'Stance'], header=0)\n",
    "baseline = pd.read_csv(BASELINE_PREDICTIONS, names=['Headline', 'Body ID', 'Stance'], header=0)\n",
    "baseline.columns = ['Headline', 'Body ID', 'Stance_baseline']\n",
    "test_res = create_merged_df(TEST_BODIES_PATH, TEST_STANCES_PATH)\n",
    "# test_res['albert'] = pd.malbert[['Headline', 'Stance']]\n",
    "test_res = pd.merge(test_res,albert, on=['Headline', 'Body ID'], suffixes=['_true', '_albert'])\n",
    "test_res = pd.merge(test_res,baseline, on=['Headline', 'Body ID'])\n",
    "train = create_merged_df(TRAIN_BODIES_PATH, TRAIN_STANCES_PATH)\n",
    "display(test_res)\n",
    "display(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A few sanity checks:\")\n",
    "correct_agree_albert = test_res.loc[(test_res['Stance_true'] == 'agree') & (test_res['Stance_albert'] == 'agree')].shape[0]\n",
    "correct_agree_base = test_res.loc[(test_res['Stance_true'] == 'agree') & (test_res['Stance_baseline'] == 'agree')].shape[0]\n",
    "print(f\"Amount of agrees, predicted by ALBERT as agree: {correct_agree_albert}\")\n",
    "print(f\"Amount of agrees, predicted by baseline as agree: {correct_agree_base}\")\n",
    "\n",
    "\n",
    "agree_disagree_albert = test_res.loc[(test_res['Stance_true'] == 'agree') & (test_res['Stance_albert'] == 'disagree')].shape[0]\n",
    "print(f\"Amount of agrees, predicted by ALBERT as disagree: {agree_disagree_albert}\")\n",
    "correct_unrelated = test_res.loc[(test_res['Stance_true'] == 'unrelated') & (test_res['Stance_baseline'] == 'unrelated')].shape[0]\n",
    "print(f\"Amount of unrelated pairs, correctly predicted by baseline as unrelated: {correct_unrelated}\")\n",
    "\n",
    "\n",
    "def get_confusion_value(true_label, predicted_label):\n",
    "    return test_res.loc[(test_res['Stance_true'] == true_label) & (test_res['Stance_albert'] == predicted_label)].shape[0]\n",
    "confusion_matrix_albert = [[get_confusion_value('agree', 'agree'),get_confusion_value('agree', 'disagree'),get_confusion_value('agree', 'discuss'),get_confusion_value('agree', 'unrelated')],\n",
    "             [test_res.loc[(test_res['Stance_true'] == 'disagree') & (test_res['Stance_albert'] == 'agree')].shape[0],\n",
    "            test_res.loc[(test_res['Stance_true'] == 'disagree') & (test_res['Stance_albert'] == 'disagree')].shape[0],\n",
    "            test_res.loc[(test_res['Stance_true'] == 'disagree') & (test_res['Stance_albert'] == 'discuss')].shape[0],\n",
    "            test_res.loc[(test_res['Stance_true'] == 'disagree') & (test_res['Stance_albert'] == 'unrelated')].shape[0]],\n",
    "             [test_res.loc[(test_res['Stance_true'] == 'discuss') & (test_res['Stance_albert'] == 'agree')].shape[0],\n",
    "            test_res.loc[(test_res['Stance_true'] == 'discuss') & (test_res['Stance_albert'] == 'disagree')].shape[0],\n",
    "            test_res.loc[(test_res['Stance_true'] == 'discuss') & (test_res['Stance_albert'] == 'discuss')].shape[0],\n",
    "            test_res.loc[(test_res['Stance_true'] == 'discuss') & (test_res['Stance_albert'] == 'unrelated')].shape[0]],\n",
    "             [test_res.loc[(test_res['Stance_true'] == 'unrelated') & (test_res['Stance_albert'] == 'agree')].shape[0],\n",
    "            test_res.loc[(test_res['Stance_true'] == 'unrelated') & (test_res['Stance_albert'] == 'disagree')].shape[0],\n",
    "            test_res.loc[(test_res['Stance_true'] == 'unrelated') & (test_res['Stance_albert'] == 'discuss')].shape[0],\n",
    "            test_res.loc[(test_res['Stance_true'] == 'unrelated') & (test_res['Stance_albert'] == 'unrelated')].shape[0]]]\n",
    "print(f\"confusion matrix for ALBERT: {confusion_matrix_albert}\")\n",
    "print(\"WARNING: there seems to be a discrepancy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_headlines = set(test_res['Headline'].values)\n",
    "train_count = 0\n",
    "for headline in train['Headline'].values:\n",
    "    if headline in test_headlines:\n",
    "        train_count += 1\n",
    "\n",
    "train_length = len(train['Headline'].values)\n",
    "print(f\"There are {train_count} train headlines that occur in the test set out of {train_length}\")\n",
    "\n",
    "\n",
    "train_headlines = set(train['Headline'].values)\n",
    "test_count = 0\n",
    "for headline in test_res['Headline'].values:\n",
    "    if headline in train_headlines:\n",
    "        test_count += 1\n",
    "test_length = len(test_res['Headline'].values)\n",
    "print(f'There are {test_count} test headlines that occur in the train set out of {test_length}')\n",
    "\n",
    "print(f'We have {len(train_headlines)} unique train headlines and {len(test_headlines)}, the intersection has size {len(train_headlines.intersection(test_headlines))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res_rel = test_res.loc[test_res['Stance_true'] != 'unrelated']\n",
    "correct = test_res_rel.copy()\n",
    "correct['correct_albert'] = test_res_rel['Stance_true'] == test_res_rel['Stance_albert']\n",
    "correct['correct_base'] = test_res_rel['Stance_true'] == test_res_rel['Stance_baseline']\n",
    "correct = correct[['articleBody', 'Headline', 'Stance_true', 'correct_albert', 'correct_base']]\n",
    "display(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_count = correct[['Headline', 'correct_albert', 'correct_base']].groupby(['Headline']).sum().sort_values('correct_albert', ascending=False)\n",
    "correct_count.reset_index(level=0, inplace=True)\n",
    "display(correct_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-madness",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean correct albert: {correct_count['correct_albert'].mean()}\")\n",
    "print(f\"Mean correct base: {correct_count['correct_base'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-employment",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pair_count = {}\n",
    "rel_headlines = set(test_res_rel['Headline'].values)\n",
    "# print(rel_headlines)\n",
    "for head in rel_headlines:\n",
    "    pair_count[head] = test_res_rel.loc[test_res_rel['Headline'] == head].shape[0]\n",
    "    \n",
    "grouped_res = correct_count.copy()\n",
    "missed_count_albert = []\n",
    "missed_count_base = []\n",
    "total = []\n",
    "\n",
    "for headline in grouped_res['Headline'].values:\n",
    "    total.append(pair_count[headline])\n",
    "    missed_count_albert.append(pair_count[headline] - grouped_res.loc[grouped_res['Headline'] == headline]['correct_albert'].values[0])\n",
    "    missed_count_base.append(pair_count[headline] - grouped_res.loc[grouped_res['Headline'] == headline]['correct_base'].values[0])\n",
    "     \n",
    "grouped_res['missed_albert'] = missed_count_albert\n",
    "grouped_res['missed_base'] = missed_count_base\n",
    "grouped_res['total'] = total\n",
    "\n",
    "grouped_res['prop_albert'] = grouped_res['correct_albert'] / grouped_res['total']\n",
    "grouped_res['prop_base'] = grouped_res['correct_base'] / grouped_res['total']\n",
    "\n",
    "grouped_res['difference'] = grouped_res['correct_albert'] - grouped_res['correct_base']\n",
    "display(grouped_res)\n",
    "# display(grouped_res.loc[grouped_res['total'] > 20].sort_values('prop_albert', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grouped_res[['Headline','difference','correct_albert','correct_base']].sort_values('difference', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-constitution",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grouped_res.sort_values(['prop_albert', 'total'], ascending=[True, False])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grouped_res.sort_values(['prop_base', 'total'], ascending=[True, False])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average score for albert: {grouped_res['prop_albert'].mean()}\")\n",
    "print(f\"Average score for baseline: {grouped_res['prop_base'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_count_albert = grouped_res.loc[grouped_res['prop_albert'] < 0.001].shape[0]\n",
    "failed_count_base = grouped_res.loc[grouped_res['prop_base'] < 0.001].shape[0]\n",
    "print(f\"Amount of headlines where models failed completely: albert: {failed_count_albert}, base: {failed_count_base}\")\n",
    "succ_count_albert = grouped_res.loc[grouped_res['prop_albert'] > 0.999].shape[0]\n",
    "succ_count_base = grouped_res.loc[grouped_res['prop_base'] > 0.999].shape[0]\n",
    "print(f\"Amount of headlines where models scored perfectly: albert: {succ_count_albert}, base: {succ_count_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-absolute",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_range = [0, 50]\n",
    "plt.hist(grouped_res['total'], bins=20, range=hist_range)\n",
    "print(f\"Amount of headlines that are outliers (larger than {hist_range[1]}): {grouped_res.loc[grouped_res['total'] > hist_range[1]].shape[0]}\")\n",
    "plt.grid()\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Amount of bodies for headline\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_range = [0,1]\n",
    "plt.hist(grouped_res['prop_albert'], bins=20, range=hist_range)\n",
    "plt.grid()\n",
    "plt.title(\"Correctly predicted bodies by ALBERT model\")\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Proportion of correctly predicted bodies per headline')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_range = [0,1]\n",
    "plt.hist(grouped_res['prop_base'], bins=20, range=hist_range)\n",
    "plt.grid()\n",
    "plt.title(\"Correctly predicted bodies by baseline model\")\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Proportion of correctly predicted bodies per headline')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_headlines_albert = set(grouped_res.loc[grouped_res['prop_albert'] > 0.99]['Headline'].values)\n",
    "correct_headlines_base = set(grouped_res.loc[grouped_res['prop_base'] > 0.99]['Headline'].values)\n",
    "failed_headlines_albert = set(grouped_res.loc[grouped_res['prop_albert'] < 0.01]['Headline'].values)\n",
    "failed_headlines_base = set(grouped_res.loc[grouped_res['prop_base'] < 0.01]['Headline'].values)\n",
    "print(f\"Intersection size of set of correct headlines for albert ({len(correct_headlines_albert)}) and baseline ({len(correct_headlines_base)}): {len(correct_headlines_albert.intersection(correct_headlines_base))}\")\n",
    "print(f\"Intersection size of set of failed headlines for albert ({len(failed_headlines_albert)}) and baseline ({len(failed_headlines_base)}): {len(failed_headlines_albert.intersection(failed_headlines_base))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = set(string.punctuation)\n",
    "sw = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize_corpus(corpus):\n",
    "    token_dict = {}\n",
    "    result = []\n",
    "    tf = {}\n",
    "    for passage in corpus:\n",
    "        tokens = word_tokenize(passage)\n",
    "        tokenized = []\n",
    "        for token in tokens:\n",
    "            if token not in sw and token not in punct:\n",
    "                stem = stemmer.stem(token.lower())\n",
    "                tokenized.append(stem)\n",
    "                token_dict[stem] = token\n",
    "                if stem not in tf.keys():\n",
    "                    tf[stem] = 0\n",
    "                tf[stem] += 1\n",
    "        result.append(tokenized)\n",
    "    return (result, token_dict, tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(x):\n",
    "    return [(k, v) for k, v in sorted(x.items(), key=lambda item: item[1], reverse=True)]\n",
    "\n",
    "tokenized_set, token_dict1, tf1 = tokenize_corpus(correct_headlines_albert)\n",
    "word_dict_albert = {}\n",
    "for stem in tf1.keys():\n",
    "    word_dict_albert[stem] = tf1[stem]\n",
    "    \n",
    "word_dict_albert = sort_dict(word_dict_albert)\n",
    "\n",
    "\n",
    "tokenized_set, token_dict2, tf2 = tokenize_corpus(correct_headlines_base)\n",
    "word_dict_base = {}\n",
    "for stem in tf2.keys():\n",
    "    word_dict_base[stem] = tf2[stem]\n",
    "    \n",
    "word_dict_base = sort_dict(word_dict_base)\n",
    "\n",
    "t=15\n",
    "\n",
    "print(f\"Top {t} frequent words in successfull headlines of ALBERT: {word_dict_albert[:t]}\")\n",
    "print(f\"Top {t} frequent words in successfull headlines of Baseline: {word_dict_base[:t]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-constitutional",
   "metadata": {},
   "source": [
    "# Intermezzo: corpus statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-survival",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-chassis",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BODIES_PATH = \"data/fnc-1/train_bodies.csv\"\n",
    "TRAIN_STANCES_PATH = \"data/fnc-1/train_stances.csv\"\n",
    "\n",
    "train_bodies = pd.read_csv(TRAIN_BODIES_PATH, names=['Body ID', 'articleBody'], header=0)\n",
    "train_stances = pd.read_csv(TRAIN_STANCES_PATH, names=['Headline', 'Body ID', 'Stance'], header=0)\n",
    "\n",
    "\n",
    "print(\"Num headlines:\", len(set(stances['Headline'])) + len(set(train_stances['Headline'])))\n",
    "print(\"Num bodies:\", len(set(bodies['articleBody'])) + len(set(train_bodies['articleBody'])))\n",
    "print(\"Num instances:\", train_stances.shape[0] + stances.shape[0])\n",
    "\n",
    "stance_count = {}\n",
    "all_stances = np.concatenate((train_stances['Stance'].values, stances['Stance'].values))\n",
    "for stance in all_stances:\n",
    "    if stance not in stance_count.keys():\n",
    "        stance_count[stance] = 0\n",
    "    else:\n",
    "        stance_count[stance] += 1\n",
    "print(stance_count)\n",
    "stance_freq = [count / len(all_stances) for count in stance_count.values()]\n",
    "print(stance_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Predictions'] = albert['Stance']\n",
    "df['Baseline'] = baseline['Stance']\n",
    "display(df.sample(n=5))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-omaha",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out only related pairs\n",
    "df_rel = df.loc[df['Stance'] != 'unrelated']\n",
    "\n",
    "display(df_rel.sample(n=5))\n",
    "print(df_rel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-january",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-slide",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Download if not installed already\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-genre",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load word2vec model\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "# stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "sw = stopwords.words('english')\n",
    "# punctuation\n",
    "punct = set(string.punctuation)\n",
    "# stemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_vectorizer = TfidfVectorizer(stop_words='english', max_features=50000)\n",
    "tfidf_matrix  = tf_vectorizer.fit_transform(df_rel['Headline'])\n",
    "#importing LDAfrom gensim import corpora, models\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "#Fitting \n",
    "lda = LatentDirichletAllocation(n_components=100, learning_method='online', \n",
    "                                          random_state=0, verbose=0, n_jobs = -1)\n",
    "lda_model = lda.fit(tfidf_matrix)\n",
    "lda_matrix = lda_model.transform(tfidf_matrix)\n",
    "lda_matrix.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = tf_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx )\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))# Print the topics found by the LDA model\n",
    "\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda_model, df_rel['Headline'], 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-relative",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# embeddings = np.zeros((7064, 300))\n",
    "# print(embeddings.shape)\n",
    "# not_found = set([])\n",
    "# for i, headline in tqdm(enumerate(df_rel['Headline'].values), total=len(df_rel['Headline'].values)):\n",
    "#     tokens = word_tokenize(headline)\n",
    "#     # Doc vec is average of summed word vectors\n",
    "#     doc_vec = np.zeros(300)\n",
    "#     n = len(tokens)\n",
    "#     for token in tokens or token in sw:\n",
    "#         if token in punct:\n",
    "#             continue\n",
    "#         try: \n",
    "#             vec = wv[token]\n",
    "#             doc_vec += vec                \n",
    "#         except KeyError: \n",
    "#             not_found.add(token)\n",
    "        \n",
    "#     doc_vec /= n\n",
    "#     embeddings[i] = np.array(doc_vec)\n",
    "#     print(embeddings[i])\n",
    "#     break\n",
    "# print(f'{len(not_found)} Tokens not found: {not_found}')\n",
    "        \n",
    "        \n",
    "### Tokenization and lemmatization    \n",
    "#     new_headline = []\n",
    "#     for token in tokens:\n",
    "#         token = token.lower()\n",
    "#         if token not in punct and token not in sw:\n",
    "#             new_headline.append(stemmer.stem(token))\n",
    "#     tokenized_headlines.append(new_headline)\n",
    "# 'th' means tokenized headlines\n",
    "# print(embeddings.shape)\n",
    "# df_rel['embeddings'] = embeddings\n",
    "# display(df_rel[['Headline', 'embeddings']].sample(n=5))\n",
    "# from sklearn.cluster import KMeans\n",
    "# print(df_rel['embeddings'].values.shape)\n",
    "# vector = np.vectorize(np.float64)\n",
    "# X = df_rel['embeddings'].values\n",
    "# for i, x in enumerate(X):\n",
    "#     X[i] = vector(x)\n",
    "# # print(df_rel['embeddings'].values.reshape(7064,300).shape)\n",
    "# print(embeddings.shape)\n",
    "# model = KMeans(n_clusters=100, verbose=10)\n",
    "# print(embeddings.shape)\n",
    "# output = model.fit(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-thermal",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-india",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_rel['Correct Albert'] = df_rel['Stance'] == df_rel['Predictions']\n",
    "df_rel['Correct Base'] = df_rel['Stance'] == df_rel['Baseline']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_rel[['Stance', 'Predictions', 'Baseline','Correct Albert', 'Correct Base']].sample(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-roulette",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_rel.groupby(['Headline','Head ID']).sum().sort_values('Correct Albert', ascending=True)\n",
    "counts = [df_rel.loc[df_rel['Headline'] == h].shape[0] for h in [i[0] for i in grouped.index]]\n",
    "# print(counts, grouped.index)\n",
    "# display(grouped)\n",
    "grouped['Total'] = counts\n",
    "grouped['Prop Albert'] = grouped['Correct Albert'] / grouped['Total']\n",
    "grouped['Prop Base'] = grouped['Correct Base'] / grouped['Total']\n",
    "display(grouped)\n",
    "print('Mean proportion albert:', grouped['Prop Albert'].mean())\n",
    "print('Mean proportion baseline:', grouped['Prop Base'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-compression",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = grouped.loc[grouped['Correct Albert'] == 0]\n",
    "success = grouped.loc[grouped['Prop Albert'] > 0.5]\n",
    "failed_headlines = [idx[0] for idx in failed.index]\n",
    "success_headlines = [idx[0] for idx in success.index]\n",
    "print(success_headlines, len(success_headlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_headlines = [(h, df_rel.loc[df_rel['Headline'] == h].shape[0]) for h in failed_headlines]\n",
    "print(failed_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-standard",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist([f[1] for f in failed_headlines], bins=30, range = [0, 60])\n",
    "plt.grid()\n",
    "plt.savefig('hist_failed.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_headlines = df_rel['Headline'].values\n",
    "all_headlines = [(h, df_rel.loc[df_rel['Headline'] == h].shape[0]) for h in all_headlines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([f[1] for f in all_headlines], bins=30, range = [0, 60])\n",
    "plt.grid()\n",
    "plt.savefig('hist_all.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_headlines = [(h, df_rel.loc[df_rel['Headline'] == h].shape[0]) for h in success_headlines]\n",
    "# print(success_headlines)\n",
    "plt.hist([f[1] for f in success_headlines], bins=30, range = [0, 60])\n",
    "plt.grid()\n",
    "plt.savefig('hist_succ.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-suffering",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
