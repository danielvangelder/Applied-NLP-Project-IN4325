{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-married",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-south",
   "metadata": {},
   "source": [
    "Reference for the code: [Fake News Detection Powered with BERT and Friends\n",
    "](https://medium.com/@vslovik/fake-news-detection-empowered-with-bert-and-friends-20397f7e1675) by Valeriya Slovikovskaya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-spokesman",
   "metadata": {},
   "source": [
    "# Using Simple Transformers\n",
    "\n",
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-canadian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnc(path_headlines, path_bodies):\n",
    "\n",
    "    stance_map = {'agree': 0, 'disagree':1, 'discuss':2, 'unrelated':3}\n",
    "\n",
    "    with open(path_bodies, encoding='utf_8') as fb:  # Body ID,articleBody\n",
    "        body_dict = {}\n",
    "        lines_b = csv.reader(fb)\n",
    "        for i, line in enumerate(tqdm(list(lines_b), ncols=80, leave=False)):\n",
    "            if i > 0:\n",
    "                body_id = int(line[0].strip())\n",
    "                body_dict[body_id] = line[1]\n",
    "\n",
    "    with open(path_headlines, encoding='utf_8') as fh: # Headline,Body ID,Stance\n",
    "        lines_h = csv.reader(fh)\n",
    "        h = []\n",
    "        b = []\n",
    "        l = []\n",
    "        for i, line in enumerate(tqdm(list(lines_h), ncols=80, leave=False)):\n",
    "            if i > 0:\n",
    "                body_id = int(line[1].strip())\n",
    "                label = line[2].strip()\n",
    "                if label in stance_map and body_id in body_dict:\n",
    "                    h.append(line[0])\n",
    "                    l.append(stance_map[line[2]])\n",
    "                    b.append(body_dict[body_id])\n",
    "    return h, b, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/fnc-1/'\n",
    "headlines, bodies, labels = fnc(\n",
    "    os.path.join(data_dir, 'train_stances.csv'),\n",
    "    os.path.join(data_dir, 'train_bodies.csv')\n",
    ")\n",
    "\n",
    "list_of_tuples = list(zip(headlines, bodies, labels))\n",
    "df = pd.DataFrame(list_of_tuples, columns=['text_a', 'text_b', 'labels'])\n",
    "train_df, val_df = train_test_split(df)\n",
    "labels_val = pd.Series(val_df['labels']).to_numpy()\n",
    "\n",
    "headlines, bodies, labels = fnc(\n",
    "    os.path.join(data_dir, 'competition_test_stances.csv'),\n",
    "    os.path.join(data_dir, 'competition_test_bodies.csv')\n",
    ")\n",
    "\n",
    "list_of_tuples = list(zip(headlines, bodies, labels))\n",
    "test_df = pd.DataFrame(list_of_tuples, columns=['text_a', 'text_b', 'labels'])\n",
    "labels_test = pd.Series(test_df['labels']).to_numpy()\n",
    "\n",
    "display(train_df.sample(n=5))\n",
    "display(test_df.sample(n=5))\n",
    "print(set(train_df['labels'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-impact",
   "metadata": {},
   "source": [
    "## Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-means",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original: 'bert', 'bert-base-uncased'\n",
    "# New: 'albert', 'albert-base-v2'\n",
    "model = ClassificationModel('albert', 'albert-base-v2', num_labels=4, use_cuda=False, args={\n",
    "    'learning_rate':1e-5,\n",
    "    'num_train_epochs': 5,\n",
    "    'reprocess_input_data': True,\n",
    "    'overwrite_output_dir': True,\n",
    "    'process_count': 10,\n",
    "    'train_batch_size': 4,\n",
    "    'eval_batch_size': 4,\n",
    "    'max_seq_length': 512,\n",
    "    'fp16': True\n",
    "})\n",
    "\n",
    "# Takes loooong...\n",
    "# model.train_model(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-merchandise",
   "metadata": {},
   "source": [
    "# Using Transformers Hugging Face\n",
    "\n",
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FncDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, maxlen, with_labels=True, bert_model='albert-base-v2'):\n",
    "        self.data = data  # pandas dataframe\n",
    "        #Initialize the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)  \n",
    "\n",
    "        self.maxlen = maxlen\n",
    "        self.with_labels = with_labels \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Selecting sentence1 and sentence2 at the specified index in the data frame\n",
    "        sent1 = str(self.data.loc[index, 'text_a'])\n",
    "        sent2 = str(self.data.loc[index, 'text_b'])\n",
    "\n",
    "        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n",
    "        encoded_pair = self.tokenizer(sent1, sent2, \n",
    "                                      padding='max_length',       # Pad to max_length\n",
    "                                      truncation=True,            # Truncate to max_length\n",
    "                                      max_length=self.maxlen,  \n",
    "                                      return_tensors='pt')        # Return torch.Tensor objects\n",
    "        \n",
    "        token_ids = encoded_pair['input_ids'].squeeze(0)  # tensor of token ids\n",
    "        attn_masks = encoded_pair['attention_mask'].squeeze(0)  # binary tensor with \"0\" for padded values and \"1\" for the other values\n",
    "        token_type_ids = encoded_pair['token_type_ids'].squeeze(0)  # binary tensor with \"0\" for the 1st sentence tokens & \"1\" for the 2nd sentence tokens\n",
    "\n",
    "        if self.with_labels:  # True if the dataset has labels\n",
    "            label = int(self.data.loc[index, 'labels'])\n",
    "            return {\"input_ids\": token_ids, 'attention_mask': attn_masks, 'token_type_ids': token_type_ids, \"label\": label  }\n",
    "        else:\n",
    "            return {\"input_ids\": token_ids, 'attention_mask': attn_masks, 'token_type_ids': token_type_ids, \"label\": label  }\n",
    "            # return token_ids, attn_masks, token_type_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "# Define bert model\n",
    "bert_model = 'albert-base-v2'\n",
    "# Create datasets\n",
    "train_dataset = FncDataset(train_df, 512, bert_model=bert_model)\n",
    "val_dataset = FncDataset(val_df, 512, bert_model=bert_model)\n",
    "test_dataset = FncDataset(test_df, 512, bert_model=bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "label_to_id = {'agree': 0, 'disagree':1, 'discuss':2, 'unrelated':3}\n",
    "id_to_label = {v:k for (k,v) in label_to_id.items()}\n",
    "\n",
    "config = AutoConfig.from_pretrained(bert_model, num_labels=4)\n",
    "model = BertForSequenceClassification.from_pretrained(bert_model, config=config)\n",
    "model.config.num_labels = 4\n",
    "model.config.id2label = id_to_label\n",
    "model.config.label2id = label_to_id\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-split",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
