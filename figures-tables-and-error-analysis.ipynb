{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "another-system",
   "metadata": {},
   "source": [
    "# Data Loading/Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "from tqdm.notebook import tqdm\n",
    "# stopwords\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-character",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_BODIES_PATH = \"data/fnc-1/competition_test_bodies.csv\"\n",
    "TEST_STANCES_PATH = \"data/fnc-1/competition_test_stances.csv\"\n",
    "TRAIN_BODIES_PATH = \"data/fnc-1/train_bodies.csv\"\n",
    "TRAIN_STANCES_PATH = \"data/fnc-1/train_stances.csv\"\n",
    "ALBERT_PREDICTIONS = \"data/fnc-1/golden_labels_2.csv\"\n",
    "BASELINE_PREDICTIONS = \"data/fnc-1/baseline_output.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-request",
   "metadata": {},
   "source": [
    "## Loading Bodies and Stances for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_merged_df(bodies_path, stances_path):\n",
    "    bodies = pd.read_csv(bodies_path, names=['Body ID', 'articleBody'], header=0)\n",
    "    stances = pd.read_csv(stances_path, names=['Headline', 'Body ID', 'Stance'], header=0)\n",
    "    df = pd.merge(bodies, stances, on='Body ID')\n",
    "    return df\n",
    "\n",
    "\n",
    "albert = pd.read_csv(ALBERT_PREDICTIONS, names=['Headline', 'Body ID', 'Stance'], header=0)\n",
    "baseline = pd.read_csv(BASELINE_PREDICTIONS, names=['Headline', 'Body ID', 'Stance'], header=0)\n",
    "baseline.columns = ['Headline', 'Body ID', 'Stance_baseline']\n",
    "test_res = create_merged_df(TEST_BODIES_PATH, TEST_STANCES_PATH)\n",
    "# test_res['albert'] = pd.malbert[['Headline', 'Stance']]\n",
    "test_res = pd.merge(test_res,albert, on=['Headline', 'Body ID'], suffixes=['_true', '_albert'])\n",
    "test_res = pd.merge(test_res,baseline, on=['Headline', 'Body ID'])\n",
    "train = create_merged_df(TRAIN_BODIES_PATH, TRAIN_STANCES_PATH)\n",
    "# display(test_res)\n",
    "# display(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A few sanity checks:\")\n",
    "correct_agree_albert = test_res.loc[(test_res['Stance_true'] == 'agree') & (test_res['Stance_albert'] == 'agree')].shape[0]\n",
    "correct_agree_base = test_res.loc[(test_res['Stance_true'] == 'agree') & (test_res['Stance_baseline'] == 'agree')].shape[0]\n",
    "print(f\"Amount of agrees, predicted by ALBERT as agree: {correct_agree_albert}\")\n",
    "print(f\"Amount of agrees, predicted by baseline as agree: {correct_agree_base}\")\n",
    "\n",
    "\n",
    "agree_disagree_albert = test_res.loc[(test_res['Stance_true'] == 'agree') & (test_res['Stance_albert'] == 'disagree')].shape[0]\n",
    "print(f\"Amount of agrees, predicted by ALBERT as disagree: {agree_disagree_albert}\")\n",
    "correct_unrelated = test_res.loc[(test_res['Stance_true'] == 'unrelated') & (test_res['Stance_baseline'] == 'unrelated')].shape[0]\n",
    "print(f\"Amount of unrelated pairs, correctly predicted by baseline as unrelated: {correct_unrelated}\")\n",
    "\n",
    "\n",
    "def get_confusion_value(true_label, predicted_label):\n",
    "    return test_res.loc[(test_res['Stance_true'] == true_label) & (test_res['Stance_albert'] == predicted_label)].shape[0]\n",
    "confusion_matrix_albert = [[get_confusion_value('agree', 'agree'),get_confusion_value('agree', 'disagree'),get_confusion_value('agree', 'discuss'),get_confusion_value('agree', 'unrelated')],\n",
    "             [test_res.loc[(test_res['Stance_true'] == 'disagree') & (test_res['Stance_albert'] == 'agree')].shape[0],\n",
    "            test_res.loc[(test_res['Stance_true'] == 'disagree') & (test_res['Stance_albert'] == 'disagree')].shape[0],\n",
    "            test_res.loc[(test_res['Stance_true'] == 'disagree') & (test_res['Stance_albert'] == 'discuss')].shape[0],\n",
    "            test_res.loc[(test_res['Stance_true'] == 'disagree') & (test_res['Stance_albert'] == 'unrelated')].shape[0]],\n",
    "             [test_res.loc[(test_res['Stance_true'] == 'discuss') & (test_res['Stance_albert'] == 'agree')].shape[0],\n",
    "            test_res.loc[(test_res['Stance_true'] == 'discuss') & (test_res['Stance_albert'] == 'disagree')].shape[0],\n",
    "            test_res.loc[(test_res['Stance_true'] == 'discuss') & (test_res['Stance_albert'] == 'discuss')].shape[0],\n",
    "            test_res.loc[(test_res['Stance_true'] == 'discuss') & (test_res['Stance_albert'] == 'unrelated')].shape[0]],\n",
    "             [test_res.loc[(test_res['Stance_true'] == 'unrelated') & (test_res['Stance_albert'] == 'agree')].shape[0],\n",
    "            test_res.loc[(test_res['Stance_true'] == 'unrelated') & (test_res['Stance_albert'] == 'disagree')].shape[0],\n",
    "            test_res.loc[(test_res['Stance_true'] == 'unrelated') & (test_res['Stance_albert'] == 'discuss')].shape[0],\n",
    "            test_res.loc[(test_res['Stance_true'] == 'unrelated') & (test_res['Stance_albert'] == 'unrelated')].shape[0]]]\n",
    "print(f\"confusion matrix for ALBERT: {confusion_matrix_albert}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_headlines = set(test_res['Headline'].values)\n",
    "train_count = 0\n",
    "for headline in train['Headline'].values:\n",
    "    if headline in test_headlines:\n",
    "        train_count += 1\n",
    "\n",
    "train_length = len(train['Headline'].values)\n",
    "print(f\"There are {train_count} train headlines that occur in the test set out of {train_length}\")\n",
    "\n",
    "\n",
    "train_headlines = set(train['Headline'].values)\n",
    "test_count = 0\n",
    "for headline in test_res['Headline'].values:\n",
    "    if headline in train_headlines:\n",
    "        test_count += 1\n",
    "test_length = len(test_res['Headline'].values)\n",
    "print(f'There are {test_count} test headlines that occur in the train set out of {test_length}')\n",
    "\n",
    "print(f'We have {len(train_headlines)} unique train headlines and {len(test_headlines)}, the intersection has size {len(train_headlines.intersection(test_headlines))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-season",
   "metadata": {},
   "source": [
    "## Discard Unrelated Instances and Map Model Stances to Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res_rel = test_res.loc[test_res['Stance_true'] != 'unrelated']\n",
    "correct = test_res_rel.copy()\n",
    "correct['correct_albert'] = test_res_rel['Stance_true'] == test_res_rel['Stance_albert']\n",
    "correct['correct_base'] = test_res_rel['Stance_true'] == test_res_rel['Stance_baseline']\n",
    "correct = correct[['articleBody', 'Headline', 'Stance_true', 'correct_albert', 'correct_base']]\n",
    "display(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-algorithm",
   "metadata": {},
   "source": [
    "## Aggregate Dataframes by Headlines and Bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_count = correct[['Headline', 'correct_albert', 'correct_base']].groupby(['Headline']).sum().sort_values('correct_albert', ascending=False)\n",
    "correct_count.reset_index(level=0, inplace=True)\n",
    "display(correct_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-round",
   "metadata": {},
   "source": [
    "# Figure 2: Performance on Aggregated Bodies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_bodies = correct[['articleBody', 'correct_albert', 'correct_base']].groupby(['articleBody']).sum().sort_values('correct_albert', ascending=False)\n",
    "grouped_bodies.reset_index(level=0, inplace=True)\n",
    "\n",
    "pair_count = {}\n",
    "rel_bodies = set(grouped_bodies['articleBody'].values)\n",
    "# print(rel_headlines)\n",
    "for body in rel_bodies:\n",
    "    pair_count[body] = test_res_rel.loc[test_res_rel['articleBody'] == body].shape[0]\n",
    "\n",
    "missed_count_albert = []\n",
    "missed_count_base = []\n",
    "total = []\n",
    "\n",
    "for body in grouped_bodies['articleBody'].values:\n",
    "    total.append(pair_count[body])\n",
    "    missed_count_albert.append(pair_count[body] - grouped_bodies.loc[grouped_bodies['articleBody'] == body]['correct_albert'].values[0])\n",
    "    missed_count_base.append(pair_count[body] - grouped_bodies.loc[grouped_bodies['articleBody'] == body]['correct_base'].values[0])\n",
    "     \n",
    "grouped_bodies['missed_albert'] = missed_count_albert\n",
    "grouped_bodies['missed_base'] = missed_count_base\n",
    "grouped_bodies['total'] = total\n",
    "grouped_bodies['prop_albert'] = grouped_bodies['correct_albert'] / grouped_bodies['total']\n",
    "grouped_bodies['prop_base'] = grouped_bodies['correct_base'] / grouped_bodies['total']\n",
    "\n",
    "grouped_bodies['difference'] = grouped_bodies['correct_albert'] - grouped_bodies['correct_base']\n",
    "display(grouped_bodies)\n",
    "hist_range = [0,1]\n",
    "plt.hist(grouped_bodies['prop_albert'], bins=20, range=hist_range)\n",
    "plt.grid()\n",
    "plt.title(\"Correctly predicted headlines by ALBERT model\")\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Proportion of correctly predicted headlines per body')\n",
    "plt.savefig('prop_albert_bodies.pdf')\n",
    "plt.show()\n",
    "print(f\"Mean props: {grouped_bodies['prop_albert'].mean()}\")\n",
    "print(f\"std props: {grouped_bodies['prop_albert'].std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(grouped_bodies['prop_base'], bins=20, range=hist_range)\n",
    "plt.grid()\n",
    "plt.title(\"Correctly predicted headlines by the Baseline model\")\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Proportion of correctly predicted headlines per body')\n",
    "plt.savefig('prop_base_bodies.pdf')\n",
    "plt.show()\n",
    "print(f\"Mean props: {grouped_bodies['prop_base'].mean()}\")\n",
    "print(f\"std props: {grouped_bodies['prop_base'].std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-madness",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean correct albert: {correct_count['correct_albert'].mean()}\")\n",
    "print(f\"Mean correct base: {correct_count['correct_base'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-israel",
   "metadata": {},
   "source": [
    "# Aggregate by Headlines and Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-employment",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pair_count = {}\n",
    "rel_headlines = set(test_res_rel['Headline'].values)\n",
    "# print(rel_headlines)\n",
    "for head in rel_headlines:\n",
    "    pair_count[head] = test_res_rel.loc[test_res_rel['Headline'] == head].shape[0]\n",
    "    \n",
    "grouped_res = correct_count.copy()\n",
    "missed_count_albert = []\n",
    "missed_count_base = []\n",
    "total = []\n",
    "\n",
    "for headline in grouped_res['Headline'].values:\n",
    "    total.append(pair_count[headline])\n",
    "    missed_count_albert.append(pair_count[headline] - grouped_res.loc[grouped_res['Headline'] == headline]['correct_albert'].values[0])\n",
    "    missed_count_base.append(pair_count[headline] - grouped_res.loc[grouped_res['Headline'] == headline]['correct_base'].values[0])\n",
    "     \n",
    "grouped_res['missed_albert'] = missed_count_albert\n",
    "grouped_res['missed_base'] = missed_count_base\n",
    "grouped_res['total'] = total\n",
    "\n",
    "grouped_res['prop_albert'] = grouped_res['correct_albert'] / grouped_res['total']\n",
    "grouped_res['prop_base'] = grouped_res['correct_base'] / grouped_res['total']\n",
    "\n",
    "grouped_res['difference'] = grouped_res['correct_albert'] - grouped_res['correct_base']\n",
    "display(grouped_res)\n",
    "# display(grouped_res.loc[grouped_res['total'] > 20].sort_values('prop_albert', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grouped_res[['Headline','difference','correct_albert','correct_base']].sort_values('difference', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-constitution",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grouped_res.sort_values(['prop_albert', 'total'], ascending=[True, False])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grouped_res.sort_values(['prop_base', 'total'], ascending=[True, False])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average score for albert: {grouped_res['prop_albert'].mean()} (sigma = {grouped_res['prop_albert'].std()})\")\n",
    "print(f\"Average score for baseline: {grouped_res['prop_base'].mean()} (sigma = {grouped_res['prop_base'].std()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_count_albert = grouped_res.loc[grouped_res['prop_albert'] < 0.001].shape[0]\n",
    "failed_count_base = grouped_res.loc[grouped_res['prop_base'] < 0.001].shape[0]\n",
    "print(f\"Amount of headlines where models failed completely: albert: {failed_count_albert}, base: {failed_count_base}\")\n",
    "succ_count_albert = grouped_res.loc[grouped_res['prop_albert'] > 0.999].shape[0]\n",
    "succ_count_base = grouped_res.loc[grouped_res['prop_base'] > 0.999].shape[0]\n",
    "print(f\"Amount of headlines where models scored perfectly: albert: {succ_count_albert}, base: {succ_count_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-absolute",
   "metadata": {},
   "source": [
    "# Appendix: Related Instances per body/headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-absolute",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_range = [0, 50]\n",
    "plt.hist(grouped_res['total'], bins=20, range=hist_range)\n",
    "print(f\"Amount of headlines that are outliers (larger than {hist_range[1]}): {grouped_res.loc[grouped_res['total'] > hist_range[1]].shape[0]}\")\n",
    "plt.grid()\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Amount of bodies for headline\")\n",
    "plt.savefig('bodycount.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_range = [0, 50]\n",
    "plt.hist(grouped_bodies['total'], bins=20, range=hist_range)\n",
    "print(f\"Amount of bodies that are outliers (larger than {hist_range[1]}): {grouped_res.loc[grouped_bodies['total'] > hist_range[1]].shape[0]}\")\n",
    "plt.grid()\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Amount of headlines for a body\")\n",
    "plt.savefig('headlinecount.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-rehabilitation",
   "metadata": {},
   "source": [
    "# Figure 1: Performance per Aggregated Headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_range = [0,1]\n",
    "plt.hist(grouped_res['prop_albert'], bins=20, range=hist_range)\n",
    "plt.grid()\n",
    "plt.title(\"Correctly predicted bodies by ALBERT model\")\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Proportion of correctly predicted bodies per headline')\n",
    "plt.savefig('prop_albert.pdf')\n",
    "plt.show()\n",
    "print(f\"Mean ALBERT: {grouped_res['prop_albert'].mean()}, std: {grouped_res['prop_albert'].std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_range = [0,1]\n",
    "plt.hist(grouped_res['prop_base'], bins=20, range=hist_range)\n",
    "plt.grid()\n",
    "plt.title(\"Correctly predicted bodies by baseline model\")\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Proportion of correctly predicted bodies per headline')\n",
    "plt.savefig('prop_base.pdf')\n",
    "plt.show()\n",
    "print(f\"Mean base: {grouped_res['prop_base'].mean()}, std: {grouped_res['prop_base'].std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-music",
   "metadata": {},
   "source": [
    "# Result Analysis (Similarity between successful/unsuccessful groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_headlines_albert = set(grouped_res.loc[grouped_res['prop_albert'] > 0.99]['Headline'].values)\n",
    "correct_headlines_base = set(grouped_res.loc[grouped_res['prop_base'] > 0.99]['Headline'].values)\n",
    "failed_headlines_albert = set(grouped_res.loc[grouped_res['prop_albert'] < 0.01]['Headline'].values)\n",
    "failed_headlines_base = set(grouped_res.loc[grouped_res['prop_base'] < 0.01]['Headline'].values)\n",
    "print(f\"Intersection size of set of correct headlines for albert ({len(correct_headlines_albert)}) and baseline ({len(correct_headlines_base)}): {len(correct_headlines_albert.intersection(correct_headlines_base))}\")\n",
    "print(f\"Intersection size of set of failed headlines for albert ({len(failed_headlines_albert)}) and baseline ({len(failed_headlines_base)}): {len(failed_headlines_albert.intersection(failed_headlines_base))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_bodies_albert = set(grouped_bodies.loc[grouped_bodies['prop_albert'] < 0.001]['articleBody'].values)\n",
    "failed_instances_headlines_albert = test_res_rel.loc[test_res_rel['Headline'].isin(failed_headlines_albert)][['Headline', 'articleBody']]\n",
    "failed_instances_bodies_albert = test_res_rel.loc[test_res_rel['articleBody'].isin(failed_bodies_albert)][['Headline', 'articleBody']]\n",
    "intersection = failed_instances_headlines_albert.merge(failed_instances_bodies_albert, 'inner', on=['Headline', 'articleBody'])\n",
    "union = failed_instances_headlines_albert.merge(failed_instances_bodies_albert, 'outer', on=['Headline', 'articleBody'])\n",
    "print(f\"Body instances size: {failed_instances_bodies_albert.shape[0]}, headline instances size: {failed_instances_headlines_albert.shape[0]}\")\n",
    "print(f\"Intersection: {intersection.shape[0]}, union: {union.shape[0]}\")\n",
    "print(f\"Jaccard similarity: {intersection.shape[0] / union.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-wealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_bodies_albert = set(grouped_bodies.loc[grouped_bodies['prop_albert'] > 0.99]['articleBody'].values)\n",
    "correct_instances_headlines_albert = test_res_rel.loc[test_res_rel['Headline'].isin(correct_headlines_albert)][['Headline', 'articleBody']]\n",
    "correct_instances_bodies_albert = test_res_rel.loc[test_res_rel['articleBody'].isin(correct_bodies_albert)][['Headline', 'articleBody']]\n",
    "intersection = correct_instances_headlines_albert.merge(correct_instances_bodies_albert, 'inner', on=['Headline', 'articleBody'])\n",
    "union = correct_instances_headlines_albert.merge(correct_instances_bodies_albert, 'outer', on=['Headline', 'articleBody'])\n",
    "print(f\"Body instances size: {correct_instances_bodies_albert.shape[0]}, headline instances size: {correct_instances_headlines_albert.shape[0]}\")\n",
    "print(f\"Intersection: {intersection.shape[0]}, union: {union.shape[0]}\")\n",
    "print(f\"Jaccard similarity: {intersection.shape[0] / union.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-portrait",
   "metadata": {},
   "source": [
    "# (Discarded) Lexical Overlap between successful and unsuccessful groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = set(string.punctuation)\n",
    "sw = stopwords.words('english')\n",
    "punct.update(['`', '\\'', '\\'s', '’', '‘', '“', '”', '“', '``',\"''\"])\n",
    "stemmer = PorterStemmer()\n",
    "def tokenize_corpus(corpus):\n",
    "    token_dict = {}\n",
    "    result = []\n",
    "    tf = {}\n",
    "    for passage in corpus:\n",
    "        tokens = word_tokenize(passage)\n",
    "        tokenized = []\n",
    "        for token in tokens:\n",
    "            token = token.lower()\n",
    "            if token not in sw and token not in punct:\n",
    "                stem = stemmer.stem(token)\n",
    "                tokenized.append(stem)\n",
    "                token_dict[stem] = token\n",
    "                if stem not in tf.keys():\n",
    "                    tf[stem] = 0\n",
    "                tf[stem] += 1\n",
    "        result.append(tokenized)\n",
    "    return (result, token_dict, tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(x):\n",
    "    return [(k, v) for k, v in sorted(x.items(), key=lambda item: item[1], reverse=True)]\n",
    "\n",
    "tokenized_set, token_dict1, tf1 = tokenize_corpus(correct_headlines_albert)\n",
    "word_dict_albert = {}\n",
    "for stem in tf1.keys():\n",
    "    word_dict_albert[token_dict1[stem]] = tf1[stem]\n",
    "    \n",
    "word_dict_albert = sort_dict(word_dict_albert)\n",
    "\n",
    "\n",
    "tokenized_set, token_dict2, tf2 = tokenize_corpus(correct_headlines_base)\n",
    "word_dict_base = {}\n",
    "for stem in tf2.keys():\n",
    "    word_dict_base[token_dict2[stem]] = tf2[stem]\n",
    "    \n",
    "word_dict_base = sort_dict(word_dict_base)\n",
    "\n",
    "t=15\n",
    "\n",
    "word_freq_albert = word_dict_albert[:t]\n",
    "word_freq_base = word_dict_base[:t]\n",
    "print(f\"Top {t} frequent words in successfull headlines of ALBERT: {word_freq_albert}\")\n",
    "print(f\"Top {t} frequent words in successfull headlines of Baseline: {word_freq_base}\")\n",
    "\n",
    "overlap = len(set([w[0] for w in word_freq_albert]).intersection(set([w[0] for w in word_freq_base]))) / t\n",
    "print(f\"The overlap of this top {t} is {overlap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized_set, token_dict1, tf1 = tokenize_corpus(failed_headlines_albert)\n",
    "word_dict_albert = {}\n",
    "for stem in tf1.keys():\n",
    "    word_dict_albert[token_dict1[stem]] = tf1[stem]\n",
    "    \n",
    "word_dict_albert = sort_dict(word_dict_albert)\n",
    "\n",
    "\n",
    "tokenized_set, token_dict2, tf2 = tokenize_corpus(failed_headlines_base)\n",
    "word_dict_base = {}\n",
    "for stem in tf2.keys():\n",
    "    word_dict_base[token_dict2[stem]] = tf2[stem]\n",
    "    \n",
    "word_dict_base = sort_dict(word_dict_base)\n",
    "\n",
    "t=15\n",
    "\n",
    "word_freq_albert = word_dict_albert[:t]\n",
    "word_freq_base = word_dict_base[:t]\n",
    "print(f\"Top {t} frequent words in failed headlines of ALBERT: {word_freq_albert}\")\n",
    "print(f\"Top {t} frequent words in failed headlines of Baseline: {word_freq_base}\")\n",
    "\n",
    "overlap = len(set([w[0] for w in word_freq_albert]).intersection(set([w[0] for w in word_freq_base]))) / t\n",
    "print(f\"The overlap of this top {t} is {overlap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grouped_res.loc[grouped_res['prop_albert'] < 0.001].sort_values('difference', ascending=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(grouped_res.loc[grouped_res['prop_base'] < 0.001].sort_values('difference', ascending=False)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-garage",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test_res_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = {}\n",
    "bodies =  list(set(test_res_rel['articleBody'].values))\n",
    "headlines = list(set(test_res_rel['Headline'].values))\n",
    "corpus = bodies + headlines\n",
    "for text in tqdm(corpus, total=len(corpus)):\n",
    "    tokenized_corpus[text] = tokenize_corpus([text])[0][0]\n",
    "    \n",
    "print(len(tokenized_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = correct.copy()\n",
    "overlap_values = []\n",
    "i = 0\n",
    "for (headline, body) in tqdm(overlap[['Headline', 'articleBody']].values, total=overlap.shape[0]):\n",
    "        headline_token_set = set(tokenized_corpus[headline])\n",
    "        body_token_set = set(tokenized_corpus[body])\n",
    "        intersect = headline_token_set.intersection(body_token_set)\n",
    "        union = headline_token_set.union(body_token_set)\n",
    "        overl = len(intersect) / (len(headline_token_set))\n",
    "#         if i < 2:\n",
    "#             print(\"###########\",i)\n",
    "#             print(headline)\n",
    "#             print(headline_token_set)\n",
    "#             print(body)\n",
    "#             print(body_token_set)\n",
    "#             print(overl)\n",
    "#             i += 1 \n",
    "        overlap_values.append(overl)\n",
    "overlap['overlap'] = overlap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-committee",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-browse",
   "metadata": {},
   "source": [
    "# (Discarded) Investigation whether Lexical Overlap between headline and body is correlated to performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(overlap.loc[overlap['correct_albert'] == True]['overlap'], bins = 20)\n",
    "plt.title(\"Proportion of headline tokens present in the body for related correctly predicted instances (ALBERT)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(overlap.loc[overlap['correct_albert'] == False]['overlap'], bins = 20)\n",
    "plt.title(\"Proportion of headline tokens present in the body for related falsely predicted instances (ALBERT)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# falsely_agr = []\n",
    "# falsely_dsc = []\n",
    "# falsely_dsg = []\n",
    "# for headline in grouped_res['Headline'].values:\n",
    "#     instances = test_res_rel.loc[(test_res_rel['Headline'] == headline)]\n",
    "#     falsely_agr.append(instances.loc[intance['Stance_albert']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-condition",
   "metadata": {},
   "source": [
    "# (Discarded) Jaccard Similarity Between Succesful and Unsuccesful groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_bodies = grouped_bodies.loc[grouped_bodies['prop_albert'] < 0.001]['articleBody'].values\n",
    "failed_headlines = grouped_res.loc[grouped_res['prop_albert'] < 0.001]['Headline'].values\n",
    "instances_bodies_failed = test_res_rel.loc[test_res_rel['articleBody'].isin(failed_bodies)][['Headline', 'articleBody']]\n",
    "instances_headlines_failed = test_res_rel.loc[test_res_rel['Headline'].isin(failed_headlines)][['Headline', 'articleBody']]\n",
    "print(instances_bodies_failed.shape)\n",
    "print(instances_headlines_failed.shape)\n",
    "\n",
    "print(pd.merge(instances_bodies_failed, instances_headlines_failed).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "success_instances_albert_headlines = test_res_rel.loc[test_res_rel['Headline'].isin(correct_headlines_albert)][['Headline', 'articleBody']]\n",
    "success_instances_albert_bodies = test_res_rel.loc[test_res_rel['articleBody'].isin(correct_bodies_albert)][['Headline', 'articleBody']]\n",
    "success_merged_dfs = pd.merge(success_instances_albert_headlines,success_instances_albert_bodies,how='inner',on=['Headline', 'articleBody']).values\n",
    "success_corpus =  list(chain.from_iterable(success_merged_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_instances_albert_headlines = test_res_rel.loc[test_res_rel['Headline'].isin(failed_headlines_albert)][['Headline', 'articleBody']]\n",
    "failed_instances_albert_bodies = test_res_rel.loc[test_res_rel['articleBody'].isin(failed_bodies_albert)][['Headline', 'articleBody']]\n",
    "failed_merged_dfs = pd.merge(failed_instances_albert_headlines,failed_instances_albert_bodies,how='inner',on=['Headline', 'articleBody']).values\n",
    "failed_corpus =  list(chain.from_iterable(failed_merged_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-campaign",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_success = sort_dict(tokenize_corpus(success_corpus)[2])\n",
    "tf_failed = sort_dict(tokenize_corpus(failed_corpus)[2])\n",
    "# tf_success = sort_dict(tokenize_corpus(correct_headlines_albert)[2])\n",
    "# tf_failed = sort_dict(tokenize_corpus(failed_headlines_albert)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf_success[:15])\n",
    "print(tf_failed[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = [5, 10, 15, 20, 25, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 500, 1000, 5000]\n",
    "print(\"Jaccard similarity for\")\n",
    "for t in T:\n",
    "    success_set = set([tf[0] for tf in tf_success[:t]])\n",
    "    failed_set = set([tf[0] for tf in tf_failed[:t]])\n",
    "    print(f\"   - top {t} tokens: {len(success_set.intersection(failed_set))/len(success_set.union(failed_set))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-potato",
   "metadata": {},
   "source": [
    "# Appendix: Difficult Headlines/Body Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-fashion",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_body_row = grouped_bodies.loc[(grouped_bodies['prop_albert'] < 0.001) & (grouped_bodies['total'] > 5)].loc[889]#.loc[grouped_bodies['Body ID'] == 2557]\n",
    "# display(example_body_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Example Body:\")\n",
    "example_body = example_body_row['articleBody']\n",
    "print(example_body)\n",
    "associated_headlines = test_res_rel.loc[test_res_rel['articleBody'] == example_body]\n",
    "print(f\"Associated headlines n={associated_headlines.shape[0]}:\")\n",
    "display(associated_headlines)\n",
    "print(associated_headlines['Headline'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_headline_row = grouped_res.loc[(grouped_res['prop_albert'] < 0.001) & (grouped_res['total'] < 10)].sample(n=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-tribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(example_headline_row)\n",
    "print(f\"Example Headline:\")\n",
    "example_headline = example_headline_row['Headline'].values[0]\n",
    "print(example_headline)\n",
    "associated_bodies = test_res_rel.loc[test_res_rel['Headline'] == example_headline]\n",
    "print(f\"Associated bodies n={associated_bodies.shape[0]}:\")\n",
    "display(associated_bodies[['articleBody', 'Stance_true', 'Stance_albert', 'Stance_baseline']])\n",
    "print([body[:100] for body in associated_bodies['articleBody']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-constitutional",
   "metadata": {},
   "source": [
    "# Corpus Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-chassis",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bodies = pd.read_csv(TRAIN_BODIES_PATH, names=['Body ID', 'articleBody'], header=0)\n",
    "train_stances = pd.read_csv(TRAIN_STANCES_PATH, names=['Headline', 'Body ID', 'Stance'], header=0)\n",
    "\n",
    "test_bodies = pd.read_csv(TEST_BODIES_PATH, names=['Body ID', 'articleBody'], header=0)\n",
    "test_stances = pd.read_csv(TEST_STANCES_PATH, names=['Headline', 'Body ID', 'Stance'], header=0)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Num headlines:\", len(set(test_stances['Headline'])) + len(set(train_stances['Headline'])))\n",
    "print(\"Num bodies:\", len(set(test_bodies['articleBody'].values)) + len(set(train_bodies['articleBody'].values)))\n",
    "print(\"Num instances:\", train_stances.shape[0] + test_stances.shape[0])\n",
    "\n",
    "stance_count = {}\n",
    "all_stances = np.concatenate((train_stances['Stance'].values, test_stances['Stance'].values))\n",
    "for stance in all_stances:\n",
    "    if stance not in stance_count.keys():\n",
    "        stance_count[stance] = 0\n",
    "    else:\n",
    "        stance_count[stance] += 1\n",
    "print(stance_count)\n",
    "stance_freq = [count / len(all_stances) for count in stance_count.values()]\n",
    "print(stance_freq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
